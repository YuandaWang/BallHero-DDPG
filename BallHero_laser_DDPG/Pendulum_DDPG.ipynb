{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Continuous Pendulum with DDPG\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from DDPG_Agent import DDPG_Agent\n",
    "import gym\n",
    "\n",
    "settings = {\n",
    "    'num_state': 3,\n",
    "    'num_action':1,\n",
    "    'DDPG':{\n",
    "        'USECONV':False,\n",
    "        'learning_rate_A':0.01,\n",
    "        'learning_rate_C':0.01,\n",
    "        'gamma':0.99,\n",
    "        'batch_size':32,\n",
    "        'replay_buffer_size':5000,\n",
    "        'soft_update_rate':0.1,\n",
    "        'num_fc1':20,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-27 22:43:32,807] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1 reward: -1016.22840257 2.98817307787\n",
      "episode: 2 reward: -1207.16716338 2.95843951933\n",
      "episode: 3 reward: -1269.51087902 2.92900182199\n",
      "episode: 4 reward: -1742.20765909 2.89985704193\n",
      "episode: 5 reward: -1636.23370012 2.87100226448\n",
      "episode: 6 reward: -967.039368502 2.84243460401\n",
      "episode: 7 reward: -1087.05941913 2.81415120358\n",
      "episode: 8 reward: -1307.80664755 2.78614923469\n",
      "episode: 9 reward: -1285.38179781 2.75842589698\n",
      "episode: 10 reward: -1435.80547323 2.73097841794\n",
      "episode: 11 reward: -1037.24654797 2.70380405268\n",
      "episode: 12 reward: -1380.74427675 2.6769000836\n",
      "episode: 13 reward: -1081.17515288 2.65026382015\n",
      "episode: 14 reward: -1422.03296545 2.62389259853\n",
      "episode: 15 reward: -1824.63547509 2.59778378148\n",
      "episode: 16 reward: -1413.08664528 2.57193475797\n",
      "episode: 17 reward: -934.539846998 2.54634294293\n",
      "episode: 18 reward: -1622.39251324 2.52100577705\n",
      "episode: 19 reward: -1372.63832078 2.49592072645\n",
      "episode: 20 reward: -1789.06422246 2.47108528248\n",
      "episode: 21 reward: -1404.95315239 2.44649696145\n",
      "episode: 22 reward: -1766.38360064 2.42215330439\n",
      "episode: 23 reward: -1521.16733857 2.3980518768\n",
      "episode: 24 reward: -1374.54610293 2.37419026838\n",
      "episode: 25 reward: -1078.96155923 2.35056609284\n",
      "episode: 26 reward: -1065.1631982 2.32717698761\n",
      "episode: 27 reward: -1216.07579984 2.30402061367\n",
      "episode: 28 reward: -1433.80628857 2.28109465522\n",
      "episode: 29 reward: -1092.46339814 2.25839681955\n",
      "episode: 30 reward: -1651.54047008 2.23592483674\n",
      "episode: 31 reward: -1334.23058914 2.21367645945\n",
      "episode: 32 reward: -1643.31827019 2.19164946272\n",
      "episode: 33 reward: -1544.9441803 2.16984164373\n",
      "episode: 34 reward: -1105.7752244 2.14825082155\n",
      "episode: 35 reward: -1180.14612332 2.12687483699\n",
      "episode: 36 reward: -1799.75581636 2.10571155232\n",
      "episode: 37 reward: -974.363293806 2.0847588511\n",
      "episode: 38 reward: -1195.00350659 2.06401463791\n",
      "episode: 39 reward: -1760.47991501 2.04347683824\n",
      "episode: 40 reward: -1229.57885257 2.02314339816\n",
      "episode: 41 reward: -1624.7852256 2.00301228423\n",
      "episode: 42 reward: -780.919513079 1.98308148321\n",
      "episode: 43 reward: -1269.93050308 1.96334900191\n",
      "episode: 44 reward: -1836.41042693 1.94381286697\n",
      "episode: 45 reward: -1492.95959175 1.92447112465\n",
      "episode: 46 reward: -1307.92113507 1.90532184066\n",
      "episode: 47 reward: -1099.52661934 1.88636309998\n",
      "episode: 48 reward: -1344.88617302 1.86759300661\n",
      "episode: 49 reward: -1776.28167245 1.84900968344\n",
      "episode: 50 reward: -1354.60289511 1.83061127202\n",
      "episode: 51 reward: -1724.53498692 1.81239593241\n",
      "episode: 52 reward: -1123.81982911 1.79436184297\n",
      "episode: 53 reward: -1224.69729951 1.77650720017\n",
      "episode: 54 reward: -1049.76294686 1.75883021846\n",
      "episode: 55 reward: -756.135340249 1.74132913004\n",
      "episode: 56 reward: -1709.80296714 1.72400218468\n",
      "episode: 57 reward: -1675.02258622 1.7068476496\n",
      "episode: 58 reward: -1370.18216543 1.68986380924\n",
      "episode: 59 reward: -1656.49990493 1.67304896512\n",
      "episode: 60 reward: -1693.1277818 1.65640143566\n",
      "episode: 61 reward: -765.581288197 1.639919556\n",
      "episode: 62 reward: -1335.003735 1.62360167787\n",
      "episode: 63 reward: -1273.18546029 1.60744616937\n",
      "episode: 64 reward: -1234.68897529 1.59145141487\n",
      "episode: 65 reward: -1550.41821878 1.5756158148\n",
      "episode: 66 reward: -1823.19937661 1.55993778549\n",
      "episode: 67 reward: -1809.69132041 1.54441575907\n",
      "episode: 68 reward: -1535.97719511 1.52904818323\n",
      "episode: 69 reward: -1088.1767921 1.51383352113\n",
      "episode: 70 reward: -1692.54002059 1.49877025122\n",
      "episode: 71 reward: -1398.95966903 1.48385686707\n",
      "episode: 72 reward: -1490.33292065 1.46909187726\n",
      "episode: 73 reward: -1233.03036886 1.45447380522\n",
      "episode: 74 reward: -1298.677173 1.44000118904\n",
      "episode: 75 reward: -1117.76141861 1.42567258137\n",
      "episode: 76 reward: -981.059830255 1.41148654929\n",
      "episode: 77 reward: -1077.95914815 1.39744167409\n",
      "episode: 78 reward: -1012.4204507 1.38353655121\n",
      "episode: 79 reward: -1198.92558326 1.36976979006\n",
      "episode: 80 reward: -999.005269862 1.35614001387\n",
      "episode: 81 reward: -1480.07913824 1.3426458596\n",
      "episode: 82 reward: -1705.04065497 1.32928597775\n",
      "episode: 83 reward: -1843.77437367 1.31605903225\n",
      "episode: 84 reward: -1439.21498093 1.30296370033\n",
      "episode: 85 reward: -953.091067183 1.28999867239\n",
      "episode: 86 reward: -1143.75164139 1.27716265183\n",
      "episode: 87 reward: -888.287421188 1.26445435499\n",
      "episode: 88 reward: -1774.61283835 1.25187251096\n",
      "episode: 89 reward: -1446.46332319 1.23941586149\n",
      "episode: 90 reward: -1478.90760536 1.22708316083\n",
      "episode: 91 reward: -1254.92571931 1.21487317565\n",
      "episode: 92 reward: -1686.07980974 1.20278468487\n",
      "episode: 93 reward: -1737.74370047 1.19081647958\n",
      "episode: 94 reward: -1684.0826253 1.17896736288\n",
      "episode: 95 reward: -869.81463543 1.1672361498\n",
      "episode: 96 reward: -1250.54639701 1.15562166714\n",
      "episode: 97 reward: -880.606173659 1.14412275339\n",
      "episode: 98 reward: -1201.90950187 1.13273825859\n",
      "episode: 99 reward: -1206.96084441 1.12146704422\n",
      "episode: 100 reward: -1533.918212 1.1103079831\n",
      "episode: 101 reward: -642.409205734 1.09925995925\n",
      "episode: 102 reward: -1496.21460739 1.08832186782\n",
      "episode: 103 reward: -1397.02919938 1.07749261491\n",
      "episode: 104 reward: -1337.00585643 1.06677111756\n",
      "episode: 105 reward: -1061.42829019 1.05615630354\n",
      "episode: 106 reward: -1638.75648579 1.04564711131\n",
      "episode: 107 reward: -1073.27127051 1.0352424899\n",
      "episode: 108 reward: -1130.88367113 1.02494139877\n",
      "episode: 109 reward: -1273.02655581 1.01474280776\n",
      "episode: 110 reward: -1619.00546649 1.00464569695\n",
      "episode: 111 reward: -1277.08292468 0.994649056572\n",
      "episode: 112 reward: -970.866478698 0.984751886902\n",
      "episode: 113 reward: -1156.39980126 0.974953198165\n",
      "episode: 114 reward: -1081.59937525 0.965252010435\n",
      "episode: 115 reward: -1423.5463256 0.955647353538\n",
      "episode: 116 reward: -1222.91122068 0.94613826695\n",
      "episode: 117 reward: -1225.2904018 0.936723799709\n",
      "episode: 118 reward: -1292.23179673 0.927403010313\n",
      "episode: 119 reward: -927.383425001 0.918174966627\n",
      "episode: 120 reward: -769.359188175 0.909038745794\n",
      "episode: 121 reward: -1417.47323174 0.899993434139\n",
      "episode: 122 reward: -1363.45190494 0.891038127078\n",
      "episode: 123 reward: -1457.54190468 0.882171929027\n",
      "episode: 124 reward: -1521.48165024 0.873393953316\n",
      "episode: 125 reward: -1234.07666336 0.864703322095\n",
      "episode: 126 reward: -1064.67790788 0.856099166251\n",
      "episode: 127 reward: -1363.44192663 0.847580625317\n",
      "episode: 128 reward: -1221.63554709 0.83914684739\n",
      "episode: 129 reward: -979.529104074 0.830796989044\n",
      "episode: 130 reward: -1315.55851833 0.822530215243\n",
      "episode: 131 reward: -1705.205015 0.814345699261\n",
      "episode: 132 reward: -1241.25310989 0.806242622601\n",
      "episode: 133 reward: -1585.39795555 0.798220174906\n",
      "episode: 134 reward: -1192.12678031 0.790277553885\n",
      "episode: 135 reward: -1611.87344455 0.782413965229\n",
      "episode: 136 reward: -1087.68822427 0.774628622534\n",
      "episode: 137 reward: -1330.90594592 0.766920747221\n",
      "episode: 138 reward: -1715.7824027 0.759289568456\n",
      "episode: 139 reward: -1363.70222407 0.751734323078\n",
      "episode: 140 reward: -1142.9342164 0.744254255517\n",
      "episode: 141 reward: -1789.38025934 0.736848617723\n",
      "episode: 142 reward: -932.607623354 0.72951666909\n",
      "episode: 143 reward: -1700.2621724 0.722257676379\n",
      "episode: 144 reward: -1706.24528239 0.71507091365\n",
      "episode: 145 reward: -1556.00172928 0.707955662183\n",
      "episode: 146 reward: -1305.2094739 0.700911210412\n",
      "episode: 147 reward: -1109.92280755 0.693936853851\n",
      "episode: 148 reward: -1234.05921328 0.687031895024\n",
      "episode: 149 reward: -1667.66850463 0.680195643395\n",
      "episode: 150 reward: -1460.35913561 0.673427415297\n",
      "episode: 151 reward: -1704.86995357 0.66672653387\n",
      "episode: 152 reward: -1734.26433279 0.660092328986\n",
      "episode: 153 reward: -1784.01580381 0.653524137185\n",
      "episode: 154 reward: -1097.24632551 0.64702130161\n",
      "episode: 155 reward: -984.542719645 0.64058317194\n",
      "episode: 156 reward: -1303.83706662 0.634209104324\n",
      "episode: 157 reward: -1132.57905123 0.627898461319\n",
      "episode: 158 reward: -1198.2093935 0.621650611822\n",
      "episode: 159 reward: -992.683622195 0.615464931013\n",
      "episode: 160 reward: -1737.30399037 0.609340800288\n",
      "episode: 161 reward: -1320.54147825 0.603277607197\n",
      "episode: 162 reward: -1521.16003747 0.597274745386\n",
      "episode: 163 reward: -1487.04329313 0.591331614534\n",
      "episode: 164 reward: -1236.12820248 0.585447620293\n",
      "episode: 165 reward: -1150.43571716 0.57962217423\n",
      "episode: 166 reward: -1050.55622088 0.573854693765\n",
      "episode: 167 reward: -1288.63437072 0.568144602118\n",
      "episode: 168 reward: -1429.32216441 0.562491328244\n",
      "episode: 169 reward: -1358.12257154 0.556894306786\n",
      "episode: 170 reward: -1740.95586965 0.551352978006\n",
      "episode: 171 reward: -1179.14646146 0.545866787741\n",
      "episode: 172 reward: -1400.01347814 0.540435187339\n",
      "episode: 173 reward: -1719.87422667 0.535057633608\n",
      "episode: 174 reward: -1300.53344766 0.529733588762\n",
      "episode: 175 reward: -1071.91539117 0.524462520365\n",
      "episode: 176 reward: -1740.53766786 0.51924390128\n",
      "episode: 177 reward: -1057.69398313 0.514077209614\n",
      "episode: 178 reward: -1740.51461014 0.508961928668\n",
      "episode: 179 reward: -1684.46013401 0.503897546883\n",
      "episode: 180 reward: -1130.19676833 0.498883557793\n",
      "episode: 181 reward: -930.95621799 0.49391945997\n",
      "episode: 182 reward: -1575.83874704 0.489004756973\n",
      "episode: 183 reward: -888.523035053 0.484138957305\n",
      "episode: 184 reward: -1528.27121367 0.479321574357\n",
      "episode: 185 reward: -1169.80451548 0.474552126362\n",
      "episode: 186 reward: -995.128306721 0.469830136349\n",
      "episode: 187 reward: -1279.20637225 0.46515513209\n",
      "episode: 188 reward: -1182.80116567 0.460526646058\n",
      "episode: 189 reward: -1421.50037288 0.455944215377\n",
      "episode: 190 reward: -967.337528732 0.451407381777\n",
      "episode: 191 reward: -1573.1507382 0.446915691549\n",
      "episode: 192 reward: -1101.83329211 0.442468695497\n",
      "episode: 193 reward: -1.04061493002 0.438065948896\n",
      "episode: 194 reward: -1198.79772516 0.433707011445\n",
      "episode: 195 reward: -986.366306876 0.429391447225\n",
      "episode: 196 reward: -1355.19030077 0.425118824656\n",
      "episode: 197 reward: -1190.67388064 0.420888716448\n",
      "episode: 198 reward: -1685.23432158 0.416700699568\n",
      "episode: 199 reward: -1295.68717712 0.412554355188\n",
      "episode: 200 reward: -1239.55045874 0.408449268651\n",
      "episode: 201 reward: -1284.37897063 0.404385029423\n",
      "episode: 202 reward: -1189.97075538 0.400361231057\n",
      "episode: 203 reward: -1696.0176925 0.39637747115\n",
      "episode: 204 reward: -1723.3845985 0.392433351303\n",
      "episode: 205 reward: -1744.45933786 0.388528477079\n",
      "episode: 206 reward: -1246.56715926 0.38466245797\n",
      "episode: 207 reward: -1650.57922905 0.380834907351\n",
      "episode: 208 reward: -973.633031112 0.377045442445\n",
      "episode: 209 reward: -1402.47205691 0.373293684282\n",
      "episode: 210 reward: -1427.9830045 0.369579257665\n",
      "episode: 211 reward: -974.458869609 0.36590179113\n",
      "episode: 212 reward: -1768.57420231 0.362260916908\n",
      "episode: 213 reward: -970.638132733 0.358656270892\n",
      "episode: 214 reward: -945.174909434 0.355087492594\n",
      "episode: 215 reward: -1422.22763506 0.351554225118\n",
      "episode: 216 reward: -777.723018494 0.348056115115\n",
      "episode: 217 reward: -1762.54495462 0.344592812754\n",
      "episode: 218 reward: -747.272046132 0.341163971684\n",
      "episode: 219 reward: -1601.11306807 0.337769249002\n",
      "episode: 220 reward: -1618.76397988 0.334408305215\n",
      "episode: 221 reward: -1116.11537481 0.33108080421\n",
      "episode: 222 reward: -996.32996817 0.327786413216\n",
      "episode: 223 reward: -1193.87072574 0.324524802776\n",
      "episode: 224 reward: -1692.78065256 0.321295646709\n",
      "episode: 225 reward: -1247.36118302 0.318098622081\n",
      "episode: 226 reward: -1747.10229206 0.314933409172\n",
      "episode: 227 reward: -1586.14848357 0.31179969144\n",
      "episode: 228 reward: -944.450203935 0.308697155496\n",
      "episode: 229 reward: -1264.01611275 0.305625491069\n",
      "episode: 230 reward: -1598.34114742 0.302584390974\n",
      "episode: 231 reward: -1297.70336482 0.299573551083\n",
      "episode: 232 reward: -1744.40802874 0.296592670294\n",
      "episode: 233 reward: -1137.62296422 0.293641450503\n",
      "episode: 234 reward: -1732.23881653 0.29071959657\n",
      "episode: 235 reward: -1349.76228326 0.287826816293\n",
      "episode: 236 reward: -1074.79440392 0.284962820376\n",
      "episode: 237 reward: -1726.5950287 0.282127322404\n",
      "episode: 238 reward: -1338.86800472 0.27932003881\n",
      "episode: 239 reward: -1341.83948183 0.276540688849\n",
      "episode: 240 reward: -1176.2217007 0.27378899457\n",
      "episode: 241 reward: -1711.46299071 0.271064680788\n",
      "episode: 242 reward: -1266.61742107 0.268367475056\n",
      "episode: 243 reward: -1376.30416876 0.265697107636\n",
      "episode: 244 reward: -1412.86329184 0.263053311477\n",
      "episode: 245 reward: -1201.53027711 0.260435822184\n",
      "episode: 246 reward: -1273.76462574 0.257844377992\n",
      "episode: 247 reward: -877.750707148 0.255278719743\n",
      "episode: 248 reward: -1566.71040206 0.252738590854\n",
      "episode: 249 reward: -1026.87103466 0.250223737299\n",
      "episode: 250 reward: -1080.94076481 0.247733907578\n",
      "episode: 251 reward: -1710.54123121 0.245268852692\n",
      "episode: 252 reward: -1221.90394262 0.242828326122\n",
      "episode: 253 reward: -1320.2366901 0.240412083802\n",
      "episode: 254 reward: -1080.27774908 0.238019884093\n",
      "episode: 255 reward: -856.861809182 0.23565148776\n",
      "episode: 256 reward: -1648.16568623 0.233306657951\n",
      "episode: 257 reward: -868.917871384 0.230985160169\n",
      "episode: 258 reward: -1094.08531423 0.22868676225\n",
      "episode: 259 reward: -871.4301328 0.226411234341\n",
      "episode: 260 reward: -715.34493947 0.224158348877\n",
      "episode: 261 reward: -1152.91234285 0.221927880556\n",
      "episode: 262 reward: -1754.81321465 0.219719606317\n",
      "episode: 263 reward: -1748.80090442 0.21753330532\n",
      "episode: 264 reward: -752.263602921 0.215368758923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a61801e584ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0msigma\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99995\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuanda/Learning/DualNet_DDPG/DDPG_Agent.pyc\u001b[0m in \u001b[0;36mtrain_it\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# training critic and actor and soft update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# train critic network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# here the A should be assign with stored batch a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuanda/Learning/DualNet_DDPG/DDPG_Agent.pyc\u001b[0m in \u001b[0;36mbuffer_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0ms_batch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0ma_batch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mr_batch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuanda/anaconda2/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test with pendulum\n",
    "env = gym.make('Pendulum-v0')\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "RL = DDPG_Agent(settings, sess)\n",
    "sigma = 3\n",
    "rlog = []\n",
    "for episode in range(500):\n",
    "    s = env.reset()\n",
    "    rsum = 0\n",
    "    for step in range(200):\n",
    "        action = sess.run(RL.A, feed_dict={RL.S:[s]})[0]\n",
    "        action = [np.clip(np.random.normal(action, sigma), -1, 1)]\n",
    "        s1, r, d, info = env.step(action)\n",
    "        rsum += r\n",
    "        RL.buffer_add(s, action, r, s1, d)\n",
    "        if RL.buffer_ready:\n",
    "            RL.train_it()\n",
    "            sigma *= 0.99995\n",
    "        if d: break\n",
    "        s = s1\n",
    "    # End of episode\n",
    "    rlog.append(rsum)\n",
    "    if RL.buffer_ready:\n",
    "        print 'episode:', episode, 'reward:', rsum, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(rlog)\n",
    "# demostration\n",
    "for episode in range(1000):\n",
    "    s = env.reset()\n",
    "    for step in range(200):\n",
    "        action = sess.run(RL.A, feed_dict={RL.S:[s]})[0]\n",
    "        s, r, d, info = env.step(action)\n",
    "        env.render()\n",
    "        if d: break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
